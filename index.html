<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Open-world Multimodal Tagging, Tag Sequence Generation, Retrieval-Augmented Generation,  Order Prompting, Long-tail Label Modeling, Unseen Tag Generation">
  <meta name="keywords" content="Open-world Multimodal Tagging, Tag Sequence Generation, Retrieval-Augmented Generation,  Order Prompting, Long-tail Label Modeling, Unseen Tag Generation">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Open-Tag: A Generative Framework for Open-World Multimodal Tagging</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://keunhong.com">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://createbenchmark.github.io/">
            CREATE
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Open-Tag: A Generative Framework for Open-World Multimodal Tagging</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=igML-F8AAAAJ&hl=en&oi=ao">Ziqi Zhang</a><sup>1,2</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=qtdueToAAAAJ&hl=en">Zongyang Ma</a><sup>1,2</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=359fifkAAAAJ&hl=en&oi=ao">Peijin Wang</a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=tddnkNYAAAAJ&hl=en&oi=ao">Bing Li</a><sup>1,2</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=_qPX-hcAAAAJ&hl=en">Chunfeng Yuan</a><sup>1,2</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?hl=en&user=Wl4tl4QAAAAJ">Weiming Hu</a><sup>1,2</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Chinese Academy of Sciences Institute of Automation</span>
            <span class="author-block"><sup>2</sup>University of Chinese Academy of Sciences</span>
            <span class="author-block"><sup>3</sup>Aerospace Information Research Institute, Chinese Academy of Sciences</span>
          </div>

        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="./static/images/paradigms.jpg" alt="Teaser Image" style="width:100%;"/>
      <h2 class="subtitle has-text-centered">
        Three Paradigms for Multimodal Tagging: (a) closed-set classification with fixed labels, (b) open-vocabulary matching with candidate tag texts, <b>(c) open-world generation of free-form tag sequences.</b>
      </h2>
    </div>
  </div>
</section>



<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Multimodal tagging is essential for content understanding by assigning concise, semantically relevant tags to visual inputs. However, real-world tagging is inherently open-ended: user-generated content is noisy, long-tailed, and continuously evolving, challenging conventional closed-set or open-vocabulary classification methods. We propose <b>Open-Tag</b>, a generative framework for <b>Open-world Multimodal Tagging</b> that produces unordered, variable-length tag sequences in natural language without relying on predefined tag sets.
          </p>
          <p>
            Open-Tag introduces two key innovations: (1) an <b>Order-Prompted Tag Sequence Generation</b> that maps learnable, order-agnostic queries to latent tag semantics, enabling permutation-invariant tag generation, and (2) a <b>Multi-Source Retrieval-Augmented Generation</b> that fuses tag candidates from heterogeneous retrieval systems across visual, textual, and metadata modalities. A score normalization and aggregation strategy ensures robust fusion, enhancing the diversity and grounding of generated tags.
          </p>
          <p>
            To evaluate Open-Tag, we construct two large-scale datasets: <b>CREATE-Tag</b> (Chinese video) and <b>PEXEL-Tag</b> (English image), with over 3M videos and 160K images with tens of thousands of real-user tags. We propose a novel open-set evaluation metric, <b>Tag Gain</b>, to quantify the generation of relevant but previously unseen tags. Experiments show that Open-Tag outperforms state-of-the-art baselines on closed-set F1 and open-set Tag Gain, highlighting its generalization and novel tag discovery capabilities.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">

    <!-- Method. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h1 class="title is-3">#1 Method</h1>
        <div class="Open-Tag-method-overview">
          <img src="./static/images/open-tag-model.jpg" alt="Model Image" style="max-width:100%;"/>
        </div>
        <p>
            Overview of the Open-Tag Framework:  (a) <b>Multimodal Hybrid Encoder</b> fuses visual (image frames or video clips) and textual (user titles) features via cross-modal attention to produce a unified representation. (b) <b>Order Prompt Encoder</b> generates sample-specific learnable queries, aligned with ground-truth tags via bipartite matching and contrastive learning. (c) <b>Multi-source Tag Recommendation</b> module retrieves tag candidates from similar multimodal content via multiple retrieval systems, forming an external prompt for generation. (d) <b>Prompt-guided Tag Decoder</b> auto-regressively generates tag sequences using implicit semantics from order prompts and explicit knowledge from retrieved tags. All components are jointly trained end-to-end.
          </p>
      </div>
    </div>
    <!--/ Method. -->


    <div class="columns is-centered">

      <!-- Visual Effects. -->
      <div class="column">
        <div class="content">
          <h2 class="title is-3">Promp-Tag Alignment</h2>
          <p>
            We perform one-to-one alignment with ground-truth tags via bipartite matching, to semantically match each order prompt toward a distinct tag.
          </p>
          <img src="./static/images/tag_prompt_matching.jpg" alt="Model Image" style="max-width:100%;"/>
          </video>
        </div>
      </div>
      <!--/ Visual Effects. -->

      <!-- Matting. -->
      <div class="column">
        <h2 class="title is-3">Multi-source Video Retrieval</h2>
        <div class="columns is-centered">
          <div class="column content">
            <p>
               Given a query video, multiple retrieval systems–video-to-video, title-to-title, and cross-modal retrieval—-return relevant videos with associated tags and scores. Retrieved results are aggregated, and a consensus scoring mechanism selects top-ranked tags as explicit generation guidance.
            </p>
            <img src="./static/images/multi-scource-retrieval.jpg" alt="Model Image" style="max-width:100%;"/>
            </video>
          </div>

        </div>
      </div>
    </div>
    <!--/ Matting. -->

    <!-- Evaluation. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h1 class="title is-3">#2 Evaluation</h1>

        <!-- Datasets. -->
        <h2 class="title is-4">Datasets</h2>
        <div class="content has-text-justified">
          <p>
             We focus on <b>CREATE-Tag</b>, which provides <b> 210K training </b> <a href="https://pan.quark.cn/s/9c7982f9b2a4" target="_blank" rel="noopener">[Data Download]</a> and <b> 5K test </b> <a href="https://pan.quark.cn/s/f22121a3ac4f" target="_blank" rel="noopener">[Data Download]</a> samples. The training and test sets share 2,795 tags. For evaluation, we split these into 705 common and 2,090 rare tags.
          </p>
          <p>
              We also build <b>PEXEL-Tag</b>, a newly collected dataset from the <a href="https://www.pexels.com/" target="_blank" rel="noopener">Pexels platform </a>, comprising <b>162K training</b> <a href="https://pan.quark.cn/s/cb21cafebd4b" target="_blank" rel="noopener">[Data Download] </a> and <b>5K test</b> <a href="https://pan.quark.cn/s/538eee776f7f" target="_blank" rel="noopener">[Data Download]</a> samples. The dataset contains 28,094 unique tags, with 5,669 shared between training and testing. We further categorize these into 1,627 common and 4,042 rare tags for performance analysis
          </p>
          <img src="./static/images/create-overview.png" alt="Results Image" style="max-width:100%;"/>

          <!-- Open-set Metric -->
          <h2 class="title is-4">Open-set Metric (Tag Gain)</h2>
          To evaluate the model’s ability to generate novel and informative tags, we introduce an open-set evaluation protocol based on <i>Tag Gain</i>, which jointly considers tag novelty, visual relevance, and semantic diversity.
          </p>

          <p>
          Given a generated tag set \(T_{gen} = \{t_1, t_2, \dots, t_k\}\) and the corresponding visual input \(v\), we construct a filtered subset \(\hat{T}_{gen}\) by enforcing two criteria:
          </p>

          <ul>
            <li><b>Visual relevance:</b>
              Each candidate tag \(t_j \in T_{gen}\) must be sufficiently aligned with the image content. We measure this using the CLIP-based similarity between the tag and the visual input.<br>
              \[
                \mathrm{sim}_{\text{CLIP}}(t_j, v) > \tau_{\text{rel}}.
              \]
            </li>
            <li><b>Semantic diversity:</b>
              To avoid redundancy, each selected tag should be semantically dissimilar to the tags already included in \(\hat{T}_{gen}\). This is enforced by requiring:<br>
              \[
                \forall t' \in \hat{T}_{gen},\quad \mathrm{sim}_{\text{BGE}}(t_j, t') < \tau_{\text{div}}.
              \]
            </li>
          </ul>

          <p>
          We apply a greedy filtering process: starting from an empty set \(\hat{T}_{gen} = \emptyset\), we iterate through the candidate tags (optionally sorted by visual relevance), and sequentially add a tag \(t_j\) to \(\hat{T}_{gen}\) only if it satisfies both conditions above. Typically, we set \(\tau_{\text{rel}}=0.3\) and \(\tau_{\text{div}}=0.8\). Formally:
          </p>
          <p>
          \[
              \hat{T}_{gen} = \left\{ t_j \in T_{gen} \;\middle|\;
              \mathrm{sim}_{\text{CLIP}}(t_j, v) > \tau_{\text{rel}} \land
              \forall t' \in \hat{T}_{gen},\ \mathrm{sim}_{\text{BGE}}(t_j, t') < \tau_{\text{div}}
              \right\}.
          \]
          </p>

          <p>
          Based on the filtered tag set \(\hat{T}_{gen}\), we define two variants of <i>Tag Gain</i> to assess open-world tagging:
          </p>
          <ul>
            <li><b>Known Tag Gain</b> (\(\Delta_{\text{known}}\)): The proportion of relevant but unannotated tags that were seen during training:
              <br>
              \[
              T_{\text{known}} = \left\{ t \in \hat{T}_{gen} \;\middle|\; t \notin T_{\text{gt}},\ t \in T_{\text{train}} \right\},
              \]
              \[
              \Delta_{\text{known}} = \frac{ \left| T_{\text{known}} \right| }{ \left| T_{\text{gt}} \right| }.
              \]
            </li>
            <li><b>Novel Tag Gain</b> (\(\Delta_{\text{novel}}\)): The proportion of generated tags that are unseen in training:
              <br>
              \[
              T_{\text{novel}} = \left\{ t \in \hat{T}_{gen} \;\middle|\; \ t \notin T_{\text{train}} \right\},
              \]
              \[
              \Delta_{\text{novel}} = \frac{ \left| T_{\text{novel}} \right| }{ \left| T_{\text{gt}} \right| }.
              \]
            </li>
          </ul>
        </div>
      </div>
    </div>
    <!--/ Evaluation. -->

    <!-- Results. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h1 class="title is-3">#3 Results</h1>

        <!-- Open-set Result. -->
        <h2 class="title is-4">Open-Set Evaluation</h2>
        <table class="table is-bordered is-striped is-narrow is-hoverable is-fullwidth" style="font-size:small;">
          <thead>
            <tr>
              <th rowspan="2">Method</th>
              <th colspan="4">CREATE-Tag</th>
              <th colspan="4">PEXEL-Tag</th>
            </tr>
            <tr>
              <th>T<sub>known</sub></th>
              <th>&Delta;<sub>known</sub></th>
              <th>T<sub>novel</sub></th>
              <th>&Delta;<sub>novel</sub></th>
              <th>T<sub>known</sub></th>
              <th>&Delta;<sub>known</sub></th>
              <th>T<sub>novel</sub></th>
              <th>&Delta;<sub>novel</sub></th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>Bin</td><td>1.2</td><td>28.1%</td><td>0.00</td><td>0.00%</td><td>0.8</td><td>21.4%</td><td>0.00</td><td>0.00%</td>
            </tr>
            <tr>
              <td>ASL</td><td>1.4</td><td>32.5%</td><td>0.00</td><td>0.00%</td><td>0.9</td><td>24.6%</td><td>0.00</td><td>0.00%</td>
            </tr>
            <tr>
              <td>Order-Free</td><td>1.4</td><td>31.6%</td><td>0.00</td><td>0.00%</td><td>1.0</td><td>27.4%</td><td>0.00</td><td>0.00%</td>
            </tr>
            <tr>
              <td>Orderless</td><td>1.8</td><td>41.3%</td><td>0.00</td><td>0.00%</td><td>1.4</td><td>36.4%</td><td>0.00</td><td>0.00%</td>
            </tr>
            <tr>
              <td>Tag2Text</td><td>1.7</td><td>38.5%</td><td>0.00</td><td>0.00%</td><td>1.1</td><td>30.2%</td><td>0.00</td><td>0.00%</td>
            </tr>
            <tr>
              <td>ML-Decoder</td><td>1.8</td><td>42.2%</td><td>0.00</td><td>0.00%</td><td>1.2</td><td>33.8%</td><td>0.00</td><td>0.00%</td>
            </tr>
            <tr>
              <td>RAM</td><td>1.8</td><td>42.5%</td><td>0.00</td><td>0.00%</td><td>1.3</td><td>34.2%</td><td>0.00</td><td>0.00%</td>
            </tr>
            <tr>
              <td>RAM++</td><td>2.0</td><td>45.6%</td><td>0.00</td><td>0.00%</td><td>1.4</td><td>36.7%</td><td>0.00</td><td>0.00%</td>
            </tr>
            <tr>
              <td>Open-Book</td><td>1.6</td><td>37.1%</td><td>0.15</td><td>3.43%</td><td>1.2</td><td>31.9%</td><td>0.13</td><td>3.11%</td>
            </tr>
            <tr style="background-color:#f5f5f5;">
              <td>Baseline</td><td>1.6</td><td>37.3%</td><td>0.10</td><td>2.24%</td><td>1.3</td><td>34.7%</td><td>0.11</td><td>2.63%</td>
            </tr>
            <tr style="background-color:#f5f5f5;">
              <td>&nbsp;&nbsp;+ OPG</td><td>2.2</td><td>52.3%</td><td>0.30</td><td>7.04%</td><td>2.0</td><td>49.1%</td><td>0.24</td><td>5.66%</td>
            </tr>
            <tr style="background-color:#f5f5f5;">
              <td>&nbsp;&nbsp;+ RAG</td>
              <td><b>4.8</b></td><td><b>80.3%</b></td><td><b>3.95</b></td><td><b>72.8%</b></td>
              <td><b>4.5</b></td><td><b>94.3%</b></td><td><b>3.61</b></td><td><b>71.2%</b></td>
            </tr>
          </tbody>
        </table>
        <!-- /Open-set Result. -->

        <!-- Qualitative Examples on CREATE-Tag. -->
        <h2 class="title is-4">Qualitative Examples on CREATE-Tag</h2>
        <div class="content has-text-justified">
          <img src="./static/images/CREATE_example.jpg" alt="Results Image" style="max-width:100%;"/>
        </div>
        <!-- /Qualitative Examples on CREATE-Tag. -->

        <!-- Qualitative Examples on PEXEL-Tag. -->
        <h2 class="title is-4">Qualitative Examples on PEXEL-Tag</h2>
        <div class="content has-text-justified">
          <img src="./static/images/Pexel_example.jpg" alt="Results Image" style="max-width:100%;"/>
        </div>
        <!-- /Qualitative Examples on PEXEL-Tag. -->
        
      </div>
    </div>
    <!--/ Results. -->

  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="#">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="#" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
      <a href="https://clustrmaps.com/site/1c7ev"  title="ClustrMaps"><img src="//www.clustrmaps.com/map_v2.png?d=a-4jkXp8j9b85N9XVQdxWL7Tl-_x3-6-1Oqa9KBvLbM&cl=ffffff" /></a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
